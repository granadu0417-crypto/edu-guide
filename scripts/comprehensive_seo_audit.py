#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¢…í•© SEO ê°ì‚¬ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  ë¬¸ì œë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤
"""

from pathlib import Path
import re
from collections import defaultdict, Counter

class SEOAuditor:
    def __init__(self, content_dir='content'):
        self.content_dir = Path(content_dir)
        self.issues = defaultdict(list)
        self.stats = defaultdict(int)

    def audit_file(self, file_path):
        """íŒŒì¼ë³„ SEO ê°ì‚¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Front matter íŒŒì‹±
            if not content.startswith('---'):
                self.issues['no_frontmatter'].append(str(file_path))
                return

            parts = content.split('---', 2)
            if len(parts) < 3:
                self.issues['invalid_frontmatter'].append(str(file_path))
                return

            front_matter = parts[1]
            body = parts[2]

            rel_path = str(file_path.relative_to(self.content_dir))

            # 1. Title ì²´í¬
            title_match = re.search(r'^title:\s*(.+)$', front_matter, re.MULTILINE)
            if not title_match:
                self.issues['missing_title'].append(rel_path)
            else:
                title = title_match.group(1).strip()
                if len(title) < 10:
                    self.issues['title_too_short'].append(f"{rel_path} ({len(title)}ì)")
                if len(title) > 60:
                    self.issues['title_too_long'].append(f"{rel_path} ({len(title)}ì)")
                if title.count('|') > 2:
                    self.issues['title_too_many_separators'].append(rel_path)

            # 2. Description ì²´í¬
            desc_match = re.search(r'^description:\s*(.+?)(?=\n[a-z_]+:|$)', front_matter, re.MULTILINE | re.DOTALL)
            if not desc_match:
                self.issues['missing_description'].append(rel_path)
            else:
                desc = desc_match.group(1).strip()
                if len(desc) < 50:
                    self.issues['description_too_short'].append(f"{rel_path} ({len(desc)}ì)")
                if len(desc) > 160:
                    self.issues['description_too_long'].append(f"{rel_path} ({len(desc)}ì)")

            # 3. Featured Image ì²´í¬
            image_match = re.search(r'^featured_image:\s*(.+)$', front_matter, re.MULTILINE)
            if not image_match:
                self.issues['missing_featured_image'].append(rel_path)

            # 4. Tags ì²´í¬ (ìˆ˜ì •ëœ ì •ê·œì‹)
            tags_match = re.search(r'^tags:\s*\n((?:- .+\n?)+)', front_matter, re.MULTILINE)
            if not tags_match:
                self.issues['missing_tags'].append(rel_path)
            else:
                tags_content = tags_match.group(1)
                tag_count = len(re.findall(r'^-\s+', tags_content, re.MULTILINE))
                if tag_count < 3:
                    self.issues['tags_too_few'].append(f"{rel_path} ({tag_count}ê°œ)")
                if tag_count > 15:
                    self.issues['tags_too_many'].append(f"{rel_path} ({tag_count}ê°œ)")

            # 5. Categories ì²´í¬ (ìˆ˜ì •ëœ ì •ê·œì‹)
            cat_match = re.search(r'^categories:\s*\n((?:- .+\n?)+)', front_matter, re.MULTILINE)
            if not cat_match:
                self.issues['missing_categories'].append(rel_path)

            # 6. Body ì½˜í…ì¸  ì²´í¬
            body_text = re.sub(r'#+\s+', '', body)  # í—¤ë”© ì œê±°
            body_text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', body_text)  # ë§í¬ í…ìŠ¤íŠ¸ë§Œ
            body_text = re.sub(r'[*_`]', '', body_text)  # ë§ˆí¬ë‹¤ìš´ ì œê±°
            word_count = len(body_text.split())

            if word_count < 300:
                self.issues['content_too_short'].append(f"{rel_path} ({word_count}ë‹¨ì–´)")

            # 7. í—¤ë”© êµ¬ì¡° ì²´í¬
            headings = re.findall(r'^(#{1,6})\s+(.+)$', body, re.MULTILINE)
            if not headings:
                self.issues['no_headings'].append(rel_path)
            else:
                h1_count = sum(1 for h in headings if h[0] == '#')
                if h1_count > 1:
                    self.issues['multiple_h1'].append(f"{rel_path} ({h1_count}ê°œ)")

            # 8. ë‚´ë¶€ ë§í¬ ì²´í¬
            internal_links = re.findall(r'\[([^\]]+)\]\((/[^\)]+)\)', body)
            if len(internal_links) < 2:
                self.issues['few_internal_links'].append(f"{rel_path} ({len(internal_links)}ê°œ)")

            # í†µê³„
            self.stats['total_files'] += 1
            self.stats['total_words'] += word_count

        except Exception as e:
            self.issues['file_read_error'].append(f"{rel_path}: {e}")

    def check_duplicates(self):
        """ì¤‘ë³µ ì½˜í…ì¸  ì²´í¬"""
        titles = defaultdict(list)
        descriptions = defaultdict(list)

        for md_file in self.content_dir.glob('**/*.md'):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                title_match = re.search(r'^title:\s*(.+)$', content, re.MULTILINE)
                if title_match:
                    title = title_match.group(1).strip()
                    titles[title].append(str(md_file.relative_to(self.content_dir)))

                desc_match = re.search(r'^description:\s*(.+?)(?=\n[a-z_]+:|$)', content, re.MULTILINE | re.DOTALL)
                if desc_match:
                    desc = desc_match.group(1).strip()
                    descriptions[desc].append(str(md_file.relative_to(self.content_dir)))
            except:
                continue

        # ì¤‘ë³µ ë°œê²¬
        for title, files in titles.items():
            if len(files) > 1:
                self.issues['duplicate_titles'].append(f"{title[:50]}... ({len(files)}ê°œ íŒŒì¼)")

        for desc, files in descriptions.items():
            if len(files) > 1 and len(desc) > 50:  # ì§§ì€ descriptionì€ ë¬´ì‹œ
                self.issues['duplicate_descriptions'].append(f"{desc[:80]}... â†’ {', '.join(files[:3])}")

    def run_audit(self):
        """ì „ì²´ ê°ì‚¬ ì‹¤í–‰"""
        print("ğŸ” ì¢…í•© SEO ê°ì‚¬ ì‹œì‘...\n")

        # 1. ê°œë³„ íŒŒì¼ ê°ì‚¬
        print("ğŸ“„ íŒŒì¼ë³„ ê°ì‚¬ ì¤‘...")
        for md_file in self.content_dir.glob('**/*.md'):
            if md_file.name.startswith('_'):
                continue
            self.audit_file(md_file)

        # 2. ì¤‘ë³µ ì²´í¬
        print("ğŸ” ì¤‘ë³µ ì½˜í…ì¸  ì²´í¬ ì¤‘...")
        self.check_duplicates()

        # 3. ê²°ê³¼ ì¶œë ¥
        self.print_report()

    def print_report(self):
        """ê°ì‚¬ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š SEO ê°ì‚¬ ê²°ê³¼")
        print("=" * 80)

        print(f"\nì „ì²´ íŒŒì¼: {self.stats['total_files']}ê°œ")
        print(f"ì „ì²´ ë‹¨ì–´: {self.stats['total_words']:,}ê°œ")
        print(f"í‰ê·  ë‹¨ì–´: {self.stats['total_words'] // max(self.stats['total_files'], 1)}ê°œ/íŒŒì¼")

        if not self.issues:
            print("\nâœ… ë¬¸ì œ ì—†ìŒ! ì™„ë²½í•©ë‹ˆë‹¤!")
            return

        print(f"\në°œê²¬ëœ ì´ìŠˆ ì¹´í…Œê³ ë¦¬: {len(self.issues)}ê°œ")

        # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
        critical = ['missing_title', 'missing_description', 'no_frontmatter']
        high = ['title_too_short', 'description_too_short', 'content_too_short', 'missing_featured_image']
        medium = ['title_too_long', 'description_too_long', 'missing_tags', 'tags_too_few']

        # Critical ì´ìŠˆ
        critical_issues = {k: v for k, v in self.issues.items() if k in critical}
        if critical_issues:
            print(f"\nğŸ”´ CRITICAL ì´ìŠˆ ({sum(len(v) for v in critical_issues.values())}ê°œ)")
            for issue_type, files in critical_issues.items():
                print(f"\n  [{issue_type}] {len(files)}ê°œ")
                for f in files[:3]:
                    print(f"    - {f}")
                if len(files) > 3:
                    print(f"    ... ì™¸ {len(files) - 3}ê°œ")

        # High ì´ìŠˆ
        high_issues = {k: v for k, v in self.issues.items() if k in high}
        if high_issues:
            print(f"\nğŸŸ¡ HIGH ì´ìŠˆ ({sum(len(v) for v in high_issues.values())}ê°œ)")
            for issue_type, files in high_issues.items():
                print(f"\n  [{issue_type}] {len(files)}ê°œ")
                for f in files[:3]:
                    print(f"    - {f}")
                if len(files) > 3:
                    print(f"    ... ì™¸ {len(files) - 3}ê°œ")

        # Medium ì´ìŠˆ
        medium_issues = {k: v for k, v in self.issues.items() if k in medium}
        if medium_issues:
            print(f"\nğŸŸ¢ MEDIUM ì´ìŠˆ ({sum(len(v) for v in medium_issues.values())}ê°œ)")
            for issue_type, files in medium_issues.items():
                print(f"\n  [{issue_type}] {len(files)}ê°œ")
                for f in files[:3]:
                    print(f"    - {f}")
                if len(files) > 3:
                    print(f"    ... ì™¸ {len(files) - 3}ê°œ")

        # ê¸°íƒ€ ì´ìŠˆ
        other_issues = {k: v for k, v in self.issues.items()
                       if k not in critical and k not in high and k not in medium}
        if other_issues:
            print(f"\nâšª ê¸°íƒ€ ì´ìŠˆ ({sum(len(v) for v in other_issues.values())}ê°œ)")
            for issue_type, files in other_issues.items():
                print(f"\n  [{issue_type}] {len(files)}ê°œ")
                if issue_type == 'duplicate_descriptions':
                    # ì¤‘ë³µ description ìƒì„¸ ì¶œë ¥
                    for item in files[:10]:  # ì²˜ìŒ 10ê°œë§Œ
                        print(f"    - {item}")
                    if len(files) > 10:
                        print(f"    ... ì™¸ {len(files) - 10}ê°œ")

        print("\n" + "=" * 80)

if __name__ == '__main__':
    auditor = SEOAuditor()
    auditor.run_audit()

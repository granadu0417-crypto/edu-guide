#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì „ì²´ ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- SEO ì „ë¬¸ê°€ ê´€ì 
- ë§ˆì¼€í„° ê´€ì 
- í”„ë¡œê·¸ë˜ë¨¸ ê´€ì 
"""

import os
import re
from pathlib import Path
from collections import defaultdict, Counter
import json

def parse_front_matter(content):
    """Front matter íŒŒì‹±"""
    if not content.startswith('---'):
        return None, content

    parts = content.split('---', 2)
    if len(parts) < 3:
        return None, content

    return parts[1].strip(), parts[2].strip()

def extract_field(front_matter, field_name):
    """Front matterì—ì„œ íŠ¹ì • í•„ë“œ ì¶”ì¶œ"""
    pattern = rf'{field_name}:\s*["\']?(.+?)["\']?\s*$'
    match = re.search(pattern, front_matter, re.MULTILINE)
    return match.group(1).strip('"\'') if match else None

def extract_tags(front_matter):
    """tags ë°°ì—´ ì¶”ì¶œ"""
    pattern = r'tags:\s*\[(.+?)\]'
    match = re.search(pattern, front_matter, re.DOTALL)
    if match:
        tags_str = match.group(1)
        tags = re.findall(r'"([^"]+)"', tags_str)
        return tags
    return []

def extract_categories(front_matter):
    """categories ë°°ì—´ ì¶”ì¶œ"""
    pattern = r'categories:\s*\[(.+?)\]'
    match = re.search(pattern, front_matter, re.DOTALL)
    if match:
        cats_str = match.group(1)
        cats = re.findall(r'"([^"]+)"', cats_str)
        return cats
    return []

def analyze_site():
    """ì „ì²´ ì‚¬ì´íŠ¸ ë¶„ì„"""
    content_dir = Path('content')

    if not content_dir.exists():
        print("âŒ content ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë°ì´í„° ìˆ˜ì§‘ êµ¬ì¡°
    data = {
        'files': [],
        'images': defaultdict(list),  # image_url -> [file_paths]
        'titles': defaultdict(list),  # title -> [file_paths]
        'descriptions': defaultdict(list),  # description -> [file_paths]
        'tags': Counter(),
        'categories': Counter(),
        'issues': {
            'seo': [],
            'content': [],
            'technical': []
        },
        'stats': {
            'total_files': 0,
            'by_category': defaultdict(int),
            'avg_content_length': 0,
            'min_content_length': float('inf'),
            'max_content_length': 0,
            'files_without_images': [],
            'files_without_description': [],
            'files_without_tags': [],
            'empty_content': [],
            'short_content': [],  # < 500 words
            'duplicate_slugs': defaultdict(list)
        }
    }

    md_files = list(content_dir.rglob('*.md'))
    print(f"ğŸ” ì´ {len(md_files)}ê°œ íŒŒì¼ ë¶„ì„ ì‹œì‘...\n")

    for md_file in md_files:
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()

            front_matter, body = parse_front_matter(content)

            if not front_matter:
                data['issues']['technical'].append({
                    'file': str(md_file),
                    'issue': 'No front matter found'
                })
                continue

            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = extract_field(front_matter, 'title')
            description = extract_field(front_matter, 'description')
            featured_image = extract_field(front_matter, 'featured_image')
            tags = extract_tags(front_matter)
            categories = extract_categories(front_matter)

            # íŒŒì¼ ì •ë³´ ì €ì¥
            file_info = {
                'path': str(md_file.relative_to(content_dir)),
                'title': title,
                'description': description,
                'image': featured_image,
                'tags': tags,
                'categories': categories,
                'content_length': len(body.strip()),
                'word_count': len(body.split())
            }

            data['files'].append(file_info)
            data['stats']['total_files'] += 1

            # ì¤‘ë³µ ê²€ì‚¬
            if featured_image:
                data['images'][featured_image].append(str(md_file.relative_to(content_dir)))
            else:
                data['stats']['files_without_images'].append(str(md_file.relative_to(content_dir)))

            if title:
                data['titles'][title].append(str(md_file.relative_to(content_dir)))

            if description:
                data['descriptions'][description].append(str(md_file.relative_to(content_dir)))
            else:
                data['stats']['files_without_description'].append(str(md_file.relative_to(content_dir)))

            if not tags:
                data['stats']['files_without_tags'].append(str(md_file.relative_to(content_dir)))

            # íƒœê·¸/ì¹´í…Œê³ ë¦¬ í†µê³„
            for tag in tags:
                data['tags'][tag] += 1
            for cat in categories:
                data['categories'][cat] += 1
                data['stats']['by_category'][cat] += 1

            # ì½˜í…ì¸  ê¸¸ì´ í†µê³„
            content_len = len(body.strip())
            data['stats']['min_content_length'] = min(data['stats']['min_content_length'], content_len)
            data['stats']['max_content_length'] = max(data['stats']['max_content_length'], content_len)

            if content_len == 0:
                data['stats']['empty_content'].append(str(md_file.relative_to(content_dir)))
            elif len(body.split()) < 200:  # 200 ë‹¨ì–´ ë¯¸ë§Œ
                data['stats']['short_content'].append(str(md_file.relative_to(content_dir)))

            # ìŠ¬ëŸ¬ê·¸ ì¤‘ë³µ ê²€ì‚¬ (ê°™ì€ ë””ë ‰í† ë¦¬ ë‚´)
            slug = md_file.stem
            parent = str(md_file.parent.relative_to(content_dir))
            data['stats']['duplicate_slugs'][f"{parent}/{slug}"].append(str(md_file.relative_to(content_dir)))

            # SEO ë¬¸ì œ ê²€ì‚¬
            if title and len(title) > 60:
                data['issues']['seo'].append({
                    'file': str(md_file.relative_to(content_dir)),
                    'issue': f'Title too long ({len(title)} chars): {title[:60]}...'
                })

            if description and len(description) > 160:
                data['issues']['seo'].append({
                    'file': str(md_file.relative_to(content_dir)),
                    'issue': f'Description too long ({len(description)} chars)'
                })

            if description and len(description) < 50:
                data['issues']['seo'].append({
                    'file': str(md_file.relative_to(content_dir)),
                    'issue': f'Description too short ({len(description)} chars)'
                })

            # ì½˜í…ì¸  í’ˆì§ˆ ê²€ì‚¬
            if 'Lorem ipsum' in body or 'TODO' in body or 'FIXME' in body:
                data['issues']['content'].append({
                    'file': str(md_file.relative_to(content_dir)),
                    'issue': 'Contains placeholder text (Lorem ipsum/TODO/FIXME)'
                })

            # ë§í¬ ê²€ì‚¬
            broken_links = re.findall(r'\[([^\]]+)\]\(([^\)]+)\)', body)
            for link_text, link_url in broken_links:
                if link_url.startswith('/') and not link_url.startswith('http'):
                    # ë‚´ë¶€ ë§í¬ - ë‚˜ì¤‘ì— ê²€ì¦ ê°€ëŠ¥
                    pass

        except Exception as e:
            data['issues']['technical'].append({
                'file': str(md_file.relative_to(content_dir)),
                'issue': f'Error parsing file: {str(e)}'
            })

    # í‰ê·  ì½˜í…ì¸  ê¸¸ì´ ê³„ì‚°
    if data['stats']['total_files'] > 0:
        total_length = sum(f['content_length'] for f in data['files'])
        data['stats']['avg_content_length'] = total_length // data['stats']['total_files']

    return data

def print_report(data):
    """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ì „ì²´ ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸")
    print("="*80)

    # 1. ê¸°ë³¸ í†µê³„
    print("\nã€1. ê¸°ë³¸ í†µê³„ã€‘")
    print(f"ì´ íŒŒì¼ ìˆ˜: {data['stats']['total_files']}ê°œ")
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ìˆ˜:")
    for cat, count in sorted(data['stats']['by_category'].items(), key=lambda x: x[1], reverse=True):
        print(f"  - {cat}: {count}ê°œ")

    print(f"\nì½˜í…ì¸  ê¸¸ì´:")
    print(f"  - í‰ê· : {data['stats']['avg_content_length']:,} bytes")
    print(f"  - ìµœì†Œ: {data['stats']['min_content_length']:,} bytes")
    print(f"  - ìµœëŒ€: {data['stats']['max_content_length']:,} bytes")

    # 2. ì¤‘ë³µ ê²€ì‚¬
    print("\n" + "="*80)
    print("ã€2. ì¤‘ë³µ ê²€ì‚¬ã€‘")

    # ì¤‘ë³µ ì´ë¯¸ì§€
    duplicate_images = {img: files for img, files in data['images'].items() if len(files) > 1}
    print(f"\nğŸ–¼ï¸  ì¤‘ë³µ ì´ë¯¸ì§€: {len(duplicate_images)}ê°œ")
    if duplicate_images:
        for img, files in sorted(duplicate_images.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
            print(f"\n  ğŸ“Œ {img}")
            print(f"  ì‚¬ìš©ëœ íŒŒì¼ ({len(files)}ê°œ):")
            for f in files[:5]:
                print(f"    - {f}")
            if len(files) > 5:
                print(f"    ... ì™¸ {len(files)-5}ê°œ")

    # ì¤‘ë³µ ì œëª©
    duplicate_titles = {title: files for title, files in data['titles'].items() if len(files) > 1}
    print(f"\nğŸ“ ì¤‘ë³µ ì œëª©: {len(duplicate_titles)}ê°œ")
    if duplicate_titles:
        for title, files in sorted(duplicate_titles.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
            print(f"\n  ğŸ“Œ \"{title}\"")
            print(f"  ì‚¬ìš©ëœ íŒŒì¼ ({len(files)}ê°œ):")
            for f in files:
                print(f"    - {f}")

    # ì¤‘ë³µ description
    duplicate_descriptions = {desc: files for desc, files in data['descriptions'].items() if len(files) > 1}
    print(f"\nğŸ“„ ì¤‘ë³µ Description: {len(duplicate_descriptions)}ê°œ")
    if duplicate_descriptions:
        for desc, files in sorted(duplicate_descriptions.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
            print(f"\n  ğŸ“Œ \"{desc[:80]}...\"")
            print(f"  ì‚¬ìš©ëœ íŒŒì¼ ({len(files)}ê°œ):")
            for f in files[:5]:
                print(f"    - {f}")
            if len(files) > 5:
                print(f"    ... ì™¸ {len(files)-5}ê°œ")

    # 3. SEO ë¬¸ì œì 
    print("\n" + "="*80)
    print("ã€3. SEO ë¬¸ì œì ã€‘")

    print(f"\nğŸ” ì´ë¯¸ì§€ ì—†ëŠ” íŒŒì¼: {len(data['stats']['files_without_images'])}ê°œ")
    if data['stats']['files_without_images']:
        for f in data['stats']['files_without_images'][:10]:
            print(f"  - {f}")
        if len(data['stats']['files_without_images']) > 10:
            print(f"  ... ì™¸ {len(data['stats']['files_without_images'])-10}ê°œ")

    print(f"\nğŸ“‹ Description ì—†ëŠ” íŒŒì¼: {len(data['stats']['files_without_description'])}ê°œ")
    if data['stats']['files_without_description']:
        for f in data['stats']['files_without_description'][:10]:
            print(f"  - {f}")
        if len(data['stats']['files_without_description']) > 10:
            print(f"  ... ì™¸ {len(data['stats']['files_without_description'])-10}ê°œ")

    print(f"\nğŸ·ï¸  Tags ì—†ëŠ” íŒŒì¼: {len(data['stats']['files_without_tags'])}ê°œ")
    if data['stats']['files_without_tags']:
        for f in data['stats']['files_without_tags'][:10]:
            print(f"  - {f}")
        if len(data['stats']['files_without_tags']) > 10:
            print(f"  ... ì™¸ {len(data['stats']['files_without_tags'])-10}ê°œ")

    print(f"\nâš ï¸  SEO ì´ìŠˆ: {len(data['issues']['seo'])}ê°œ")
    if data['issues']['seo']:
        for issue in data['issues']['seo'][:20]:
            print(f"  - {issue['file']}: {issue['issue']}")
        if len(data['issues']['seo']) > 20:
            print(f"  ... ì™¸ {len(data['issues']['seo'])-20}ê°œ")

    # 4. ì½˜í…ì¸  í’ˆì§ˆ
    print("\n" + "="*80)
    print("ã€4. ì½˜í…ì¸  í’ˆì§ˆã€‘")

    print(f"\nğŸ“­ ë¹ˆ ì½˜í…ì¸ : {len(data['stats']['empty_content'])}ê°œ")
    if data['stats']['empty_content']:
        for f in data['stats']['empty_content']:
            print(f"  - {f}")

    print(f"\nğŸ“ ì§§ì€ ì½˜í…ì¸  (< 200 ë‹¨ì–´): {len(data['stats']['short_content'])}ê°œ")
    if data['stats']['short_content']:
        for f in data['stats']['short_content'][:10]:
            print(f"  - {f}")
        if len(data['stats']['short_content']) > 10:
            print(f"  ... ì™¸ {len(data['stats']['short_content'])-10}ê°œ")

    print(f"\nâš ï¸  ì½˜í…ì¸  ì´ìŠˆ: {len(data['issues']['content'])}ê°œ")
    if data['issues']['content']:
        for issue in data['issues']['content'][:10]:
            print(f"  - {issue['file']}: {issue['issue']}")
        if len(data['issues']['content']) > 10:
            print(f"  ... ì™¸ {len(data['issues']['content'])-10}ê°œ")

    # 5. ê¸°ìˆ ì  ë¬¸ì œ
    print("\n" + "="*80)
    print("ã€5. ê¸°ìˆ ì  ë¬¸ì œã€‘")

    print(f"\nğŸ”§ ê¸°ìˆ  ì´ìŠˆ: {len(data['issues']['technical'])}ê°œ")
    if data['issues']['technical']:
        for issue in data['issues']['technical']:
            print(f"  - {issue['file']}: {issue['issue']}")

    # 6. íƒœê·¸ í†µê³„
    print("\n" + "="*80)
    print("ã€6. íƒœê·¸ í†µê³„ (ìƒìœ„ 20ê°œ)ã€‘")
    for tag, count in data['tags'].most_common(20):
        print(f"  - {tag}: {count}íšŒ")

    # 7. ì¢…í•© ì ìˆ˜
    print("\n" + "="*80)
    print("ã€7. ì¢…í•© í‰ê°€ã€‘")

    total_issues = (
        len(duplicate_images) * 2 +
        len(duplicate_titles) * 3 +
        len(duplicate_descriptions) * 2 +
        len(data['stats']['files_without_images']) * 1 +
        len(data['stats']['files_without_description']) * 2 +
        len(data['stats']['files_without_tags']) * 1 +
        len(data['stats']['empty_content']) * 5 +
        len(data['stats']['short_content']) * 1 +
        len(data['issues']['seo']) * 2 +
        len(data['issues']['content']) * 3 +
        len(data['issues']['technical']) * 4
    )

    max_score = 100
    penalty = min(total_issues, max_score)
    score = max(0, max_score - penalty)

    print(f"\nğŸ“Š ì¢…í•© ì ìˆ˜: {score}/100")
    print(f"ì´ ë¬¸ì œì : {total_issues}ê°œ")

    if score >= 90:
        grade = "ğŸŒŸ ìš°ìˆ˜"
    elif score >= 80:
        grade = "âœ… ì–‘í˜¸"
    elif score >= 70:
        grade = "âš ï¸  ë³´í†µ"
    elif score >= 60:
        grade = "ğŸ”§ ê°œì„  í•„ìš”"
    else:
        grade = "âŒ ì‹¬ê°í•œ ë¬¸ì œ"

    print(f"ë“±ê¸‰: {grade}")

    # ìš°ì„ ìˆœìœ„ ê°œì„ ì‚¬í•­
    print("\n" + "="*80)
    print("ã€8. ìš°ì„ ìˆœìœ„ ê°œì„ ì‚¬í•­ã€‘")

    priority_fixes = []

    if data['stats']['empty_content']:
        priority_fixes.append(f"ğŸ”´ ê¸´ê¸‰: {len(data['stats']['empty_content'])}ê°œ ë¹ˆ ì½˜í…ì¸  íŒŒì¼")

    if len(duplicate_titles) > 10:
        priority_fixes.append(f"ğŸŸ  ë†’ìŒ: {len(duplicate_titles)}ê°œ ì¤‘ë³µ ì œëª© ìˆ˜ì •")

    if len(data['stats']['files_without_description']) > 20:
        priority_fixes.append(f"ğŸŸ¡ ì¤‘ê°„: {len(data['stats']['files_without_description'])}ê°œ íŒŒì¼ì— Description ì¶”ê°€")

    if len(duplicate_images) > 20:
        priority_fixes.append(f"ğŸŸ¢ ë‚®ìŒ: {len(duplicate_images)}ê°œ ì¤‘ë³µ ì´ë¯¸ì§€ ë‹¤ì–‘í™”")

    if len(data['stats']['short_content']) > 50:
        priority_fixes.append(f"ğŸŸ¡ ì¤‘ê°„: {len(data['stats']['short_content'])}ê°œ ì§§ì€ ì½˜í…ì¸  ë³´ê°•")

    for i, fix in enumerate(priority_fixes, 1):
        print(f"{i}. {fix}")

    print("\n" + "="*80)

def save_detailed_report(data):
    """ìƒì„¸ ë¦¬í¬íŠ¸ JSON ì €ì¥"""
    output_file = 'site_audit_report.json'

    # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ ë°ì´í„° ë³€í™˜
    report = {
        'total_files': data['stats']['total_files'],
        'duplicate_images': {img: files for img, files in data['images'].items() if len(files) > 1},
        'duplicate_titles': {title: files for title, files in data['titles'].items() if len(files) > 1},
        'duplicate_descriptions': {desc: files for desc, files in data['descriptions'].items() if len(files) > 1},
        'files_without_images': data['stats']['files_without_images'],
        'files_without_description': data['stats']['files_without_description'],
        'files_without_tags': data['stats']['files_without_tags'],
        'empty_content': data['stats']['empty_content'],
        'short_content': data['stats']['short_content'],
        'seo_issues': data['issues']['seo'],
        'content_issues': data['issues']['content'],
        'technical_issues': data['issues']['technical'],
        'top_tags': dict(data['tags'].most_common(50)),
        'category_stats': dict(data['stats']['by_category'])
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    data = analyze_site()
    if data:
        print_report(data)
        save_detailed_report(data)

if __name__ == '__main__':
    main()
